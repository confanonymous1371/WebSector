{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk, bulk\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "_2022_policies = {}\n",
    "with open('/data/privseer/third-crawl/metadata_2023') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    if line['final']:\n",
    "        _2022_policies[line['hash']] = {'domain': urlparse(line['url']).netloc, 'url': line['url'], 'hash': line['hash'], 'pagerank': line['pagerank'], 'readability': line['readability'], 'industry': line['industry'], 'probability': line['proba'], 'industry': line['industry'], 'tracking_tech': [], 'self_regulatory_bodies': [], 'agreements_regulations': [], 'display_date': line['timestamp'].split(' ')[0], 'html_location': '/data/privseer/third-crawl/third_crawl/third_crawl/urls/'+line['folder_number']+'/'}\n",
    "\n",
    "_2019_policies = {}\n",
    "with open('/data/privseer/data/updated_privaseer_metadata1') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    _2019_policies[line['hash']] = {'display_date': '-'.join(line['crawl-date'].split('-')[::-1]), 'html_location': '/data/privseer/urls/'+line['path']+'/'}\n",
    "\n",
    "_2020_policies = {}\n",
    "with open('/data/privseer/data/updated_linkedin_metadata1') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    _2020_policies[line['file_hash']] = {'industry': line['industry'], 'display_date': '-'.join(line['crawl_date_time'].split(' ')[0].split('-')[::-1]), 'html_location': '/data/sxn5310/privacysearch/privacy_policies/pages/'+line['folder_num']+'/'}\n",
    "\n",
    "_2021_policies = {}\n",
    "with open('/data/privseer/data/2021_crawl/2021_metadata') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    if line['proba'] > 0.5 and not line['duplicate'] and not line['near_duplicate'] and line['verified']:\n",
    "        _2021_policies[line['hash']] = {'domain': urlparse(line['response']).netloc, 'url': line['response'], 'probability': line['proba'], 'display_date': line['timestamp'].split(' ')[0], 'html_location': '/data/privseer/crawl-update/crawlupdate/crawlupdate/urls/'+line['folder_number']+'/', 'vagueness': random.uniform(0, 1)}\n",
    "\n",
    "with open('/data/privseer/data/pageranks') as f:\n",
    "    data = f.readlines()\n",
    "for ranks in data:\n",
    "    ranks = ranks.strip()\n",
    "    hash_val, val = ranks.split(',')\n",
    "    if hash_val in _2019_policies:\n",
    "        _2019_policies[hash_val]['pagerank'] = val \n",
    "\n",
    "with open('/data/privseer/data/pageranks-linkedin') as f:\n",
    "    data = f.readlines()\n",
    "for ranks in data:\n",
    "    ranks = ranks.strip()\n",
    "    hash_val, val = ranks.split(',')\n",
    "    if hash_val in _2020_policies:\n",
    "        _2020_policies[hash_val]['pagerank'] = val\n",
    "\n",
    "with open('/data/privseer/data/2021_crawl/pageranks') as f:\n",
    "    data = f.readlines()\n",
    "for ranks in data:\n",
    "    ranks = ranks.strip()\n",
    "    hash_val, val = ranks.split(',')\n",
    "    if hash_val in _2021_policies:\n",
    "        _2021_policies[hash_val]['pagerank'] = val\n",
    "\n",
    "with open('/data/privseer/data/readability-output') as f:\n",
    "    data = f.readlines()\n",
    "for ranks in data:\n",
    "    ranks = ranks.strip()\n",
    "    ranks = ranks.split(',')\n",
    "    hash_val = ranks[0]\n",
    "    readability = ranks[1]\n",
    "    readability = float(readability)\n",
    "    if hash_val in _2019_policies:\n",
    "        _2019_policies[hash_val]['readability'] = readability\n",
    "\n",
    "with open('/data/privseer/data/2021_crawl/readability') as f:\n",
    "    data = f.readlines()\n",
    "for ranks in data:\n",
    "    ranks = ranks.strip()\n",
    "    hash_val, val = ranks.split(',')\n",
    "    val = float(val)\n",
    "    if hash_val in _2021_policies:\n",
    "        _2021_policies[hash_val]['readability'] = val\n",
    "\n",
    "with open('/data/sxn5310/document_classifier_output/probabilities') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    hash_val, proba = line.split(' ')\n",
    "    if hash_val in _2020_policies:\n",
    "        _2020_policies[hash_val]['probability'] = float(proba)\n",
    "\n",
    "with open('/data/privseer/python-files/probabilities') as f:\n",
    "    data = f.readlines()\n",
    "for line in data:\n",
    "    hash_val, proba = line.split(' ')\n",
    "    if hash_val in _2019_policies:\n",
    "        _2019_policies[hash_val]['probability'] = float(proba)\n",
    "\n",
    "with open('/data/privseer/data/tracking-tech.json') as f:\n",
    "    data = json.load(f)\n",
    "for i in data:\n",
    "    data[i] = set(data[i])\n",
    "for hash_val in _2019_policies:\n",
    "    _2019_policies[hash_val]['tracking_tech'] = []\n",
    "    for tech in data:\n",
    "        if hash_val in data[tech]:\n",
    "            if '-' in tech:\n",
    "                _2019_policies[hash_val]['tracking_tech'].append(''.join(tech.split('-')))\n",
    "            else:\n",
    "                _2019_policies[hash_val]['tracking_tech'].append(tech)\n",
    "\n",
    "\n",
    "with open('/data/privseer/data/2021_crawl/tracking-tech.json') as f:\n",
    "    data = json.load(f)\n",
    "for i in data:\n",
    "    data[i] = set(data[i])\n",
    "for hash_val in _2021_policies:\n",
    "    _2021_policies[hash_val]['tracking_tech'] = []\n",
    "    for tech in data:\n",
    "        if hash_val in data[tech]:\n",
    "            if '-' in tech:\n",
    "                _2021_policies[hash_val]['tracking_tech'].append(''.join(tech.split('-')))\n",
    "            else:\n",
    "                _2021_policies[hash_val]['tracking_tech'].append(tech)\n",
    "\n",
    "\n",
    "with open('/data/privseer/data/vagueness_output') as f:\n",
    "    data = f.readlines()\n",
    "vagueness = {}\n",
    "for line in data:\n",
    "    hash_, vague = line.split(' ')\n",
    "    vagueness[hash_] = float(vague)\n",
    "for hash_val in _2019_policies:\n",
    "    if hash_val in vagueness:\n",
    "        _2019_policies[hash_val]['vagueness'] = vagueness[hash_val]\n",
    "    else:\n",
    "        _2019_policies[hash_val]['vagueness'] = 0.3\n",
    "\n",
    "\n",
    "with open('/data/sxn5310/regulations-analysis/privaseer2') as f:\n",
    "    data = f.readlines()\n",
    "for hash_val in _2020_policies:\n",
    "    _2020_policies[hash_val]['agreements_regulations'] = []\n",
    "    _2020_policies[hash_val]['self_regulatory_bodies'] = []\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    if line['file_hash'] in _2020_policies:\n",
    "        if line['nai']: \n",
    "            _2020_policies[line['file_hash']]['self_regulatory_bodies'].append('nia')\n",
    "        if line['daa']:\n",
    "            _2020_policies[line['file_hash']]['self_regulatory_bodies'].append('daa')\n",
    "        if line['edaa']:\n",
    "            _2020_policies[line['file_hash']]['self_regulatory_bodies'].append('edaa')\n",
    "        if line['gdpr']:\n",
    "            _2020_policies[line['file_hash']]['agreements_regulations'].append('gdpr')\n",
    "        if line['coppa']:\n",
    "            _2020_policies[line['file_hash']]['agreements_regulations'].append('coppa')\n",
    "        if line['caloppa']:\n",
    "            _2020_policies[line['file_hash']]['agreements_regulations'].append('caloppa')\n",
    "        if line['privacyshield']:\n",
    "            _2020_policies[line['file_hash']]['agreements_regulations'].append('privacyshield')\n",
    "\n",
    "with open('/data/privseer/data/2021_crawl/regulations.json') as f:\n",
    "    data = f.readlines()\n",
    "for hash_val in _2021_policies:\n",
    "    _2021_policies[hash_val]['agreements_regulations'] = []\n",
    "    _2021_policies[hash_val]['self_regulatory_bodies'] = []\n",
    "for line in data:\n",
    "    line = json.loads(line)\n",
    "    if line['file_hash'] in _2021_policies:\n",
    "        if line['nai']:\n",
    "            _2021_policies[line['file_hash']]['self_regulatory_bodies'].append('nia')\n",
    "        if line['daa']:\n",
    "            _2021_policies[line['file_hash']]['self_regulatory_bodies'].append('daa')\n",
    "        if line['edaa']:\n",
    "            _2021_policies[line['file_hash']]['self_regulatory_bodies'].append('edaa')\n",
    "        if line['gdpr']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('gdpr')\n",
    "        if line['privacyshield']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('privacyshield')\n",
    "        if line['coppa']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('coppa')\n",
    "        if line['caloppa']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('caloppa')\n",
    "        if line['ccpa']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('ccpa')\n",
    "        if line['hipaa']:\n",
    "            _2021_policies[line['file_hash']]['agreements_regulations'].append('hippa')\n",
    "\n",
    "with open('/data/privseer/data/2021_crawl/hash_to_industry') as f:\n",
    "    _2021_industries = json.load(f)\n",
    "\n",
    "\n",
    "files = os.listdir('/data/privseer/third-crawl/boilerpipe-policy-text/')\n",
    "\n",
    "for file in tqdm(files):\n",
    "    actions = []\n",
    "    with open('/data/privseer/third-crawl/boilerpipe-policy-text/'+file, encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    for line in data:\n",
    "        line = json.loads(line)\n",
    "        _hash = line['hash']\n",
    "        if _hash in _2022_policies:\n",
    "            _2022_policies[_hash]['text'] = line['text']\n",
    "            _2022_policies[_hash]['title'] = line['title']\n",
    "            _2022_policies[_hash]['vagueness'] = 0.3\n",
    "            _2022_policies[_hash]['crawl_date'] = ['o2022']\n",
    "\n",
    "files = os.listdir('/data/privseer/linkedin-boilerpipe-policy-text/')\n",
    "\n",
    "for file in tqdm(files):\n",
    "    actions = []\n",
    "    with open('/data/privseer/linkedin-boilerpipe-policy-text/'+file, encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    for line in data:\n",
    "        line = json.loads(line)\n",
    "        _hash = line['hash']\n",
    "        if line['hash'] in _2020_policies:\n",
    "            _2020_policies[line['hash']]['crawl_date'] = ['m2020']\n",
    "            _2020_policies[line['hash']]['text'] = line['text']\n",
    "            _2020_policies[line['hash']]['title'] = line['title']\n",
    "\n",
    "\n",
    "DIRECTORY = '/data/privseer/boilerpipe-policy-text/'\n",
    "file_dump = os.listdir(DIRECTORY)\n",
    "\n",
    "for file in tqdm(file_dump):\n",
    "    with open(DIRECTORY+file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        line['industry'] = 'nan' \n",
    "        if line['hash'] in _2019_policies:\n",
    "            _2019_policies[line['hash']]['crawl_date'] = ['j2019']\n",
    "            _2019_policies[line['hash']]['text'] = line['text']\n",
    "            _2019_policies[line['hash']]['title'] = line['title']\n",
    "\n",
    "\n",
    "DIRECTORY = '/data/privseer/crawl-update/boilerpipe-policy-text/'\n",
    "file_dump = os.listdir(DIRECTORY)\n",
    "for file in tqdm(file_dump):\n",
    "    actions = []\n",
    "    with open(DIRECTORY+file) as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        line['industry'] = 'nan'\n",
    "        if line['hash'] in _2021_policies:\n",
    "            _2021_policies[line['hash']]['crawl_date'] = ['a2021']\n",
    "            _2021_policies[line['hash']]['text'] = line['text']\n",
    "            _2021_policies[line['hash']]['title'] = line['title']\n",
    "\n",
    "\n",
    "for idx, k in enumerate(_2019_policies):\n",
    "  if idx == 3: break\n",
    "  print((k, _2019_policies[k]))\n",
    "\n",
    "print()\n",
    "\n",
    "for idx, k in enumerate(_2020_policies):\n",
    "  if idx == 3: break\n",
    "  print((k, _2020_policies[k]))\n",
    "\n",
    "print()\n",
    "\n",
    "for idx, k in enumerate(_2021_policies):\n",
    "    if idx == 3: break\n",
    "    print((k, _2021_policies[k]))\n",
    "\n",
    "print()\n",
    "\n",
    "for idx, k in enumerate(_2022_policies):\n",
    "    if idx == 3: break\n",
    "    print((k, _2022_policies[k]))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
